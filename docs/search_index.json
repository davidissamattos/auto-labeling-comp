[
["index.html", "Automatic labeling comparisson - Online appendix Preface Compiling this document", " Automatic labeling comparisson - Online appendix Teodor Fredriksson, David Issa Mattos, Jan Bosch, Helena Holmstr√∂m Olsson 22 September, 2020 Preface This is the analysis of the paper The environment was defined and based on the renv package. The renv package logs all the packages in the renv.lock file and manages installation for a specific project. For more information see documentation for renv To replicate this environment, after downloading this repository, type: renv::hydrate() This command will download and install all the the packages use in this work. Note that it will install the packages only for this project. Compiling this document This document was created with the bookdown package. To compile it (and run every command to generate the models, figures and etc. ) use the custom function from the utils.R file. compile_book() "],
["rq1.html", "Chapter 1 RQ1 1.1 Descriptive statistics 1.2 Bradley terry model for ranking", " Chapter 1 RQ1 How can we rank different active learning and semi-supervised learning algorithms in terms of accuracy? d &lt;- read_csv(&#39;./data/auto-label-comparison.csv&#39;) %&gt;% dplyr::filter(ValueType==&#39;Accuracy&#39;) 1.1 Descriptive statistics p&lt;-ggplot(data=d, aes(x=Model, y=Value, fill=Model))+ geom_boxplot()+ theme(axis.text.x = element_blank())+ #remove the x labels labs(title = &#39;Accuracy (all datasets)&#39;) p save_fig(p,&#39;aggregatedboxplots.pdf&#39;) A respective table for this box-plot but with 5% and 95% quantiles d %&gt;% dplyr::group_by(Model) %&gt;% summarise(Mean = mean(Value), SD = sd(Value), Median = median(Value), &#39;5%&#39; = quantile(Value,0.05), &#39;95%&#39; = quantile(Value,0.95)) %&gt;% dplyr::ungroup() %&gt;% kable( &quot;latex&quot;, table.envir = &#39;table&#39;, caption=&#39;Summary statistics for the accuracy aggregated data&#39;, booktabs=T, label=&#39;summarystatisticstable&#39;, format.args = list(scientific = FALSE), digits = 3, linesep = &quot;&quot;) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;), full_width = F) %&gt;% readr::write_lines(&#39;./paper/summarystatisticstable.tex&#39;) Breaking down in all individual datasets if you want to justify the many outliers due to cifar p&lt;-ggplot(data=d, aes(x=Model, y=Value, fill=Model))+ geom_boxplot()+ theme(axis.text.x = element_blank())+ #remove the x labels facet_wrap(~Dataset)+ labs(title = &#39;Accuracy for individual dataset&#39;) p save_fig(p,&#39;boxplotsperdataset.pdf&#39;) 1.2 Bradley terry model for ranking To create a Bradley terry model we need first to transform our dataset to paired comparisons On each iteration for each dataset for each variable we will rank the models based on the Value of the accuracy (lower accuracy -&gt; smaller) After we expand it to wide so we can compare each algorithm with each other and create a BT dataset d_acc_rank &lt;- d %&gt;% dplyr::group_by(Dataset, Variable, iterations) %&gt;% dplyr::mutate(Rank=-rank(Value, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-Value) %&gt;% #we need to drop the Value variable to pivot wider tidyr::pivot_wider(names_from = Model, values_from=Rank) Now we can create the BT dataset #a vector with the name of the algorithms models &lt;- get_index_names_as_array(d$Model) n_models = length(models) comb &lt;- gtools::combinations(n=n_models, r=2, v=seq(1:n_models), repeats.allowed = F) #all teh paired combinations d_acc_bt &lt;- dplyr::tribble(~model0_name, ~model0, ~model1_name, ~model1, ~y, ~iterations, ~Dataset, ~DatasetType) #now we loop each row of the rank wide dataset and create a new one for(i in 1:nrow(d_acc_rank)) { current_row &lt;- d_acc_rank[i,] for(j in 1:nrow(comb)){ comb_row &lt;- comb[j,] model0_name &lt;- models[comb_row[1]] model0 &lt;- comb_row[1] model0_rank &lt;- current_row[[1,model0_name]] model1_name &lt;- models[comb_row[2]] model1 &lt;- comb_row[2] model1_rank &lt;- current_row[[1,model1_name]] diff_rank &lt;- model1_rank - model0_rank #SInce higher accuracy is better if model 1 rank- model 0 rank is positive than model1 wins and y=1 else y=0 y &lt;- ifelse(diff_rank&gt;0, 1, 0) d_acc_bt &lt;-d_acc_bt %&gt;% add_row(model0_name=model0_name, model0=model0, model1_name=model1_name, model1=model1, y=y, iterations=current_row$iterations, Dataset=current_row$Dataset, DatasetType=current_row$DatasetType) } } Now that we have the dataset we can run the model print_stan_code(&#39;./models/rankingmodel.stan&#39;) // Ranking model // Author: David Issa Mattos // Date: 6 sept 2020 // // data { int &lt;lower=1&gt; N_total; // Sample size int y[N_total]; //variable that indicates which one wins model 0 or model 1 int &lt;lower=1&gt; N_models; // Number of models int &lt;lower=1&gt; model0[N_total]; int &lt;lower=1&gt; model1[N_total]; // //To model the influence of each benchmark // int &lt;lower=1&gt; N_bm; // int bm_id[N_total]; } parameters { real a_model[N_models]; //Latent variable that represents the strength value of each model } model { real p[N_total]; a_model ~ normal(0,2); for (i in 1:N_total) { p[i] = a_model[model0[i]] - a_model[model1[i]]; } y ~ bernoulli_logit(p); } //Uncoment this part to get the posterior predictives and the log likelihood //But note that it takes a lot of space in the final model // generated quantities{ // vecor [N_total] y_rep; // vector[N_total] log_lik; // for(i in 1:N_total){ // real p; // p = a_alg[algo1[i]] - a_alg[algo0[i]]; // y_rep[i] = bernoulli_logit_rng(p); // // //Log likelihood // log_lik[i] = bernoulli_logit_lpmf(y[i] | p); // } // } m1_data &lt;- list( N_total=nrow(d_acc_bt), y = as.integer(d_acc_bt$y), N_models = as.integer(n_models), model0=as.integer(d_acc_bt$model0), model1=as.integer(d_acc_bt$model1) ) m1_fit &lt;- stan(file = &#39;./models/rankingmodel.stan&#39;, data=m1_data, chains = 4, warmup = 200, iter = 2000) saveRDS(m1_fit, file = &quot;./data/m1_fit.RDS&quot;) m1_fit &lt;-readRDS(&quot;./data/m1_fit.RDS&quot;) a_model &lt;- c(&quot;a_model[1]&quot;, &quot;a_model[2]&quot;, &quot;a_model[3]&quot;, &quot;a_model[4]&quot;, &quot;a_model[5]&quot;, &quot;a_model[6]&quot;, &quot;a_model[7]&quot;) rstan::traceplot(m1_fit, pars=a_model) hpdi &lt;- get_HPDI_from_stanfit(m1_fit) hpdi_algorithm &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_model\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=models) p&lt;-ggplot(data=hpdi_algorithm, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;Estimate&quot;, x=&quot;Model&quot;, title = &quot;HPDI of the strength of the model&quot;)+ coord_flip() p save_fig(p,&#39;strength-aggregated.pdf&#39;) Here we are extracting all samples and ranking them to have a distribution of the ranks posterior &lt;- rstan::extract(m1_fit) a_model &lt;- as_tibble(posterior$a_model) colnames(a_model) &lt;- models #sampling from the posterior s &lt;- dplyr::sample_n(a_model, size = 1000, replace=T) s &lt;- dplyr::mutate(s, rown = row_number()) wide_s &lt;- tidyr::pivot_longer(s, cols=all_of(models), names_to = &quot;Models&quot;, values_to = &quot;a_model&quot;) rank_df &lt;- wide_s %&gt;% dplyr::group_by(rown) %&gt;% dplyr::mutate(Rank = rank(-a_model, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-a_model) %&gt;% dplyr::group_by(Models) %&gt;% dplyr::summarise(MedianRank = median(Rank), VarianceRank = var(Rank)) %&gt;% dplyr::arrange(MedianRank) rank_df_table &lt;- rank_df colnames(rank_df_table) &lt;- c(&quot;Models&quot;,&quot;Median Rank&quot;, &quot;Variance of the Rank&quot;) kable(rank_df_table, &quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) Models Median Rank Variance of the Rank LabelSpreadingKNN 1 0.3863824 UncertaintySampling 2 0.4143143 QBC 3 0.2198198 RandomSampling 4 0.0118679 LabelSpreadingRBF 5 0.0000000 LabelPropagationRBF 6 0.0000000 LabelPropagationKNN 7 0.0000000 saving the table to latex rank_df_table %&gt;% kable( &quot;latex&quot;, table.envir = &#39;table&#39;, caption=&#39;Ranking of the algorithms&#39;, booktabs=T, label=&#39;rankingaggregated&#39;, format.args = list(scientific = FALSE), digits = 3, linesep = &quot;&quot;) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;), full_width = F) %&gt;% readr::write_lines(&#39;./paper/rankingaggregated.tex&#39;) "],
["rq2.html", "Chapter 2 RQ2 2.1 Descriptive statistics 2.2 Bradley terry model for ranking", " Chapter 2 RQ2 How do the rank of these algorithms change with changes in the amount of manual label effort prior to applying these methods? d &lt;- read_csv(&#39;./data/auto-label-comparison.csv&#39;) %&gt;% dplyr::filter(ValueType==&#39;Accuracy&#39;) d10 &lt;- dplyr::filter(d,Variable==&#39;10%&#39;) d50 &lt;- dplyr::filter(d,Variable==&#39;50%&#39;) 2.1 Descriptive statistics p&lt;-ggplot(data=d, aes(x=Model, y=Value, fill=Model))+ geom_boxplot()+ theme(axis.text.x = element_blank())+ #remove the x labels facet_wrap(~Variable)+ labs(title = &#39;Accuracy (all datasets)&#39;) p save_fig(p,&#39;aggregatedmanualeffortboxplots.pdf&#39;) A respective table for this box-plot but with 5% and 95% quantiles d %&gt;% dplyr::group_by(Variable,Model) %&gt;% summarise(Mean = mean(Value), SD = sd(Value), Median = median(Value), &#39;5%&#39; = quantile(Value,0.05), &#39;95%&#39; = quantile(Value,0.95)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-Variable) %&gt;% kable( &quot;latex&quot;, table.envir = &#39;table&#39;, caption=&#39;Summary statistics for the accuracy aggregated data&#39;, booktabs=T, label=&#39;summarystatisticstable&#39;, format.args = list(scientific = FALSE), digits = 3, linesep = &quot;&quot;) %&gt;% pack_rows(&quot;10% missing labels&quot;,1, n_models) %&gt;% pack_rows(&quot;50% missing labels&quot;,1+n_models,2*n_models) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;), full_width = F) %&gt;% readr::write_lines(&#39;./paper/summarystatisticstablemanualeffort.tex&#39;) 2.2 Bradley terry model for ranking To create a Bradley terry model we need first to transform our dataset to paired comparisons On each iteration for each dataset for each variable we will rank the models based on the Value of the accuracy (lower accuracy -&gt; smaller) After we expand it to wide so we can compare each algorithm with each other and create a BT dataset d_acc_rank_10 &lt;- d10 %&gt;% dplyr::group_by(Dataset, Variable, iterations) %&gt;% dplyr::mutate(Rank=-rank(Value, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-Value) %&gt;% #we need to drop the Value variable to pivot wider tidyr::pivot_wider(names_from = Model, values_from=Rank) d_acc_rank_50 &lt;- d50 %&gt;% dplyr::group_by(Dataset, Variable, iterations) %&gt;% dplyr::mutate(Rank=-rank(Value, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-Value) %&gt;% #we need to drop the Value variable to pivot wider tidyr::pivot_wider(names_from = Model, values_from=Rank) Now we can create the BT dataset #a vector with the name of the algorithms models &lt;- get_index_names_as_array(d$Model) n_models = length(models) comb &lt;- gtools::combinations(n=n_models, r=2, v=seq(1:n_models), repeats.allowed = F) #all teh paired combinations d_acc_bt_10 &lt;- dplyr::tribble(~model0_name, ~model0, ~model1_name, ~model1, ~y, ~iterations, ~Dataset, ~DatasetType) d_acc_bt_50 &lt;- dplyr::tribble(~model0_name, ~model0, ~model1_name, ~model1, ~y, ~iterations, ~Dataset, ~DatasetType) #now we loop each row of the rank wide dataset and create a new one for(i in 1:nrow(d_acc_rank_10)) { current_row &lt;- d_acc_rank_10[i,] for(j in 1:nrow(comb)){ comb_row &lt;- comb[j,] model0_name &lt;- models[comb_row[1]] model0 &lt;- comb_row[1] model0_rank &lt;- current_row[[1,model0_name]] model1_name &lt;- models[comb_row[2]] model1 &lt;- comb_row[2] model1_rank &lt;- current_row[[1,model1_name]] diff_rank &lt;- model1_rank - model0_rank #SInce higher accuracy is better if model 1 rank- model 0 rank is positive than model1 wins and y=1 else y=0 y &lt;- ifelse(diff_rank&gt;0, 1, 0) d_acc_bt_10 &lt;-d_acc_bt_10 %&gt;% add_row(model0_name=model0_name, model0=model0, model1_name=model1_name, model1=model1, y=y, iterations=current_row$iterations, Dataset=current_row$Dataset, DatasetType=current_row$DatasetType) } } #now we loop each row of the rank wide dataset and create a new one for(i in 1:nrow(d_acc_rank_50)) { current_row &lt;- d_acc_rank_50[i,] for(j in 1:nrow(comb)){ comb_row &lt;- comb[j,] model0_name &lt;- models[comb_row[1]] model0 &lt;- comb_row[1] model0_rank &lt;- current_row[[1,model0_name]] model1_name &lt;- models[comb_row[2]] model1 &lt;- comb_row[2] model1_rank &lt;- current_row[[1,model1_name]] diff_rank &lt;- model1_rank - model0_rank #SInce higher accuracy is better if model 1 rank- model 0 rank is positive than model1 wins and y=1 else y=0 y &lt;- ifelse(diff_rank&gt;0, 1, 0) d_acc_bt_50 &lt;-d_acc_bt_50 %&gt;% add_row(model0_name=model0_name, model0=model0, model1_name=model1_name, model1=model1, y=y, iterations=current_row$iterations, Dataset=current_row$Dataset, DatasetType=current_row$DatasetType) } } m10_data &lt;- list( N_total=nrow(d_acc_bt_10), y = as.integer(d_acc_bt_10$y), N_models = as.integer(n_models), model0=as.integer(d_acc_bt_10$model0), model1=as.integer(d_acc_bt_10$model1) ) m50_data &lt;- list( N_total=nrow(d_acc_bt_50), y = as.integer(d_acc_bt_50$y), N_models = as.integer(n_models), model0=as.integer(d_acc_bt_50$model0), model1=as.integer(d_acc_bt_50$model1) ) we use the same model but with different data m10_fit &lt;- stan(file = &#39;./models/rankingmodel.stan&#39;, data=m10_data, chains = 4, warmup = 200, iter = 2000) saveRDS(m10_fit, file = &quot;./data/m10_fit.RDS&quot;) m50_fit &lt;- stan(file = &#39;./models/rankingmodel.stan&#39;, data=m50_data, chains = 4, warmup = 200, iter = 2000) saveRDS(m50_fit, file = &quot;./data/m50_fit.RDS&quot;) "],
["results-for-10.html", "Chapter 3 Results for 10%", " Chapter 3 Results for 10% m10_fit &lt;-readRDS(&quot;./data/m10_fit.RDS&quot;) a_model &lt;- c(&quot;a_model[1]&quot;, &quot;a_model[2]&quot;, &quot;a_model[3]&quot;, &quot;a_model[4]&quot;, &quot;a_model[5]&quot;, &quot;a_model[6]&quot;, &quot;a_model[7]&quot;) rstan::traceplot(m10_fit, pars=a_model) hpdi &lt;- get_HPDI_from_stanfit(m10_fit) hpdi_algorithm &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_model\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=models) p&lt;-ggplot(data=hpdi_algorithm, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;Estimate&quot;, x=&quot;Model&quot;, title = &quot;HPDI of the strength of the model with 10% missing labels&quot;)+ coord_flip() p save_fig(p,&#39;strength-10.pdf&#39;) Here we are extracting all samples and ranking them to have a distribution of the ranks posterior &lt;- rstan::extract(m10_fit) a_model &lt;- as_tibble(posterior$a_model) colnames(a_model) &lt;- models #sampling from the posterior s &lt;- dplyr::sample_n(a_model, size = 1000, replace=T) s &lt;- dplyr::mutate(s, rown = row_number()) wide_s &lt;- tidyr::pivot_longer(s, cols=all_of(models), names_to = &quot;Models&quot;, values_to = &quot;a_model&quot;) rank_df &lt;- wide_s %&gt;% dplyr::group_by(rown) %&gt;% dplyr::mutate(Rank = rank(-a_model, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-a_model) %&gt;% dplyr::group_by(Models) %&gt;% dplyr::summarise(MedianRank = median(Rank), VarianceRank = var(Rank)) %&gt;% dplyr::arrange(MedianRank) rank_df_table &lt;- rank_df colnames(rank_df_table) &lt;- c(&quot;Models&quot;,&quot;Median Rank&quot;, &quot;Variance of the Rank&quot;) kable(rank_df_table, &quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) Models Median Rank Variance of the Rank UncertaintySampling 1 0.0529169 QBC 2 0.1585335 LabelSpreadingKNN 3 0.3136727 RandomSampling 4 0.3080040 LabelSpreadingRBF 5 0.0000000 LabelPropagationRBF 6 0.0000000 LabelPropagationKNN 7 0.0000000 saving the table to latex rank_df_table %&gt;% kable( &quot;latex&quot;, table.envir = &#39;table&#39;, caption=&#39;Ranking of the algorithms with 10% missing lables&#39;, booktabs=T, label=&#39;ranking10&#39;, format.args = list(scientific = FALSE), digits = 3, linesep = &quot;&quot;) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;), full_width = F) %&gt;% readr::write_lines(&#39;./paper/ranking10&#39;) "],
["results-for-50.html", "Chapter 4 Results for 50%", " Chapter 4 Results for 50% m50_fit &lt;-readRDS(&quot;./data/m50_fit.RDS&quot;) a_model &lt;- c(&quot;a_model[1]&quot;, &quot;a_model[2]&quot;, &quot;a_model[3]&quot;, &quot;a_model[4]&quot;, &quot;a_model[5]&quot;, &quot;a_model[6]&quot;, &quot;a_model[7]&quot;) rstan::traceplot(m50_fit, pars=a_model) hpdi &lt;- get_HPDI_from_stanfit(m50_fit) hpdi_algorithm &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_model\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=models) p&lt;-ggplot(data=hpdi_algorithm, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;Estimate&quot;, x=&quot;Model&quot;, title = &quot;HPDI of the strength of the model with 50% missing labels&quot;)+ coord_flip() p save_fig(p,&#39;strength-50.pdf&#39;) Here we are extracting all samples and ranking them to have a distribution of the ranks posterior &lt;- rstan::extract(m50_fit) a_model &lt;- as_tibble(posterior$a_model) colnames(a_model) &lt;- models #sampling from the posterior s &lt;- dplyr::sample_n(a_model, size = 1000, replace=T) s &lt;- dplyr::mutate(s, rown = row_number()) wide_s &lt;- tidyr::pivot_longer(s, cols=all_of(models), names_to = &quot;Models&quot;, values_to = &quot;a_model&quot;) rank_df &lt;- wide_s %&gt;% dplyr::group_by(rown) %&gt;% dplyr::mutate(Rank = rank(-a_model, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-a_model) %&gt;% dplyr::group_by(Models) %&gt;% dplyr::summarise(MedianRank = median(Rank), VarianceRank = var(Rank)) %&gt;% dplyr::arrange(MedianRank) rank_df_table &lt;- rank_df colnames(rank_df_table) &lt;- c(&quot;Models&quot;,&quot;Median Rank&quot;, &quot;Variance of the Rank&quot;) kable(rank_df_table, &quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) Models Median Rank Variance of the Rank LabelSpreadingKNN 1 0.0000000 UncertaintySampling 2 0.3403153 QBC 3 0.3580821 RandomSampling 4 0.1417578 LabelSpreadingRBF 5 0.0010000 LabelPropagationKNN 6 0.2286196 LabelPropagationRBF 7 0.2286196 saving the table to latex rank_df_table %&gt;% kable( &quot;latex&quot;, table.envir = &#39;table&#39;, caption=&#39;Ranking of the algorithms with 50% missing lables&#39;, booktabs=T, label=&#39;ranking10&#39;, format.args = list(scientific = FALSE), digits = 3, linesep = &quot;&quot;) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;), full_width = F) %&gt;% readr::write_lines(&#39;./paper/ranking50&#39;) "]
]
