[["index.html", "Automatic labeling comparison - Online appendix Preface", " Automatic labeling comparison - Online appendix Teodor Fredriksson, David Issa Mattos, Jan Bosch, Helena Holmstr√∂m Olsson 20 January, 2021 Preface This document was created with the bookdown package. To compile it (and run every command to generate the models, figures and etc. ) use the custom function from the utils.R file. compile_book() This document was compiled under the following session sessionInfo() R version 4.0.3 (2020-10-10) Platform: x86_64-apple-darwin17.0 (64-bit) Running under: macOS Big Sur 10.16 Matrix products: default LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib locale: [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 attached base packages: [1] graphics grDevices datasets utils stats methods base other attached packages: [1] gtools_3.8.2 progress_1.2.2 ggthemr_1.1.0 viridis_0.5.1 [5] viridisLite_0.3.0 patchwork_1.0.1 coda_0.19-4 rstan_2.21.2 [9] StanHeaders_2.21.0-7 glue_1.4.2 forcats_0.5.0 stringr_1.4.0 [13] dplyr_1.0.2 purrr_0.3.4 readr_1.3.1 tidyr_1.1.2 [17] tibble_3.0.4 ggplot2_3.3.3 tidyverse_1.3.0 kableExtra_1.2.1 [21] rmdformats_0.3.7 knitr_1.30 loaded via a namespace (and not attached): [1] httr_1.4.2 jsonlite_1.7.2 modelr_0.1.8 RcppParallel_5.0.2 [5] assertthat_0.2.1 highr_0.8 stats4_4.0.3 blob_1.2.1 [9] cellranger_1.1.0 yaml_2.2.1 pillar_1.4.7 backports_1.2.1 [13] lattice_0.20-41 digest_0.6.27 rvest_0.3.6 colorspace_2.0-0 [17] htmltools_0.5.1 pkgconfig_2.0.3 broom_0.7.0 haven_2.3.1 [21] bookdown_0.21 scales_1.1.1 webshot_0.5.2 processx_3.4.5 [25] farver_2.0.3 generics_0.1.0 ellipsis_0.3.1 withr_2.3.0 [29] cli_2.2.0 magrittr_2.0.1 crayon_1.3.4 readxl_1.3.1 [33] evaluate_0.14 ps_1.5.0 fs_1.5.0 fansi_0.4.1 [37] xml2_1.3.2 pkgbuild_1.2.0 tools_4.0.3 loo_2.4.1 [41] prettyunits_1.1.1 hms_0.5.3 lifecycle_0.2.0 matrixStats_0.57.0 [45] V8_3.4.0 munsell_0.5.0 reprex_0.3.0 callr_3.5.1 [49] compiler_4.0.3 rlang_0.4.10 grid_4.0.3 rstudioapi_0.13 [53] labeling_0.4.2 rmarkdown_2.6 gtable_0.3.0 codetools_0.2-16 [57] inline_0.3.17 DBI_1.1.0 curl_4.3 R6_2.5.0 [61] gridExtra_2.3 lubridate_1.7.9 stringi_1.5.3 parallel_4.0.3 [65] Rcpp_1.0.5 vctrs_0.3.6 dbplyr_1.4.4 tidyselect_1.1.0 [69] xfun_0.20 "],["rq1.html", "Chapter 1 RQ1 1.1 Descriptive statistics 1.2 Bradley terry model for ranking", " Chapter 1 RQ1 How can we rank different active learning and semi-supervised learning algorithms in terms of accuracy? d &lt;- read_csv(&#39;./data/auto-label-comparison.csv&#39;) %&gt;% dplyr::rename(Value=&#39;value&#39;) %&gt;% dplyr::filter(ValueType==&#39;accuracy&#39;) 1.1 Descriptive statistics p&lt;-ggplot(data=d, aes(x=Model, y=Value, fill=Model))+ geom_boxplot()+ theme(axis.text.x = element_blank())+ #remove the x labels labs(title = &#39;Accuracy (all datasets)&#39;) p save_fig(p,&#39;aggregatedboxplots.pdf&#39;) A respective table for this box-plot but with 5% and 95% quantiles d %&gt;% dplyr::group_by(Model) %&gt;% summarise(Mean = mean(Value), SD = sd(Value), Median = median(Value), &#39;5%&#39; = quantile(Value,0.05), &#39;95%&#39; = quantile(Value,0.95)) %&gt;% dplyr::ungroup() %&gt;% kable( &quot;latex&quot;, table.envir = &#39;table&#39;, caption=&#39;Summary statistics for the accuracy aggregated data&#39;, booktabs=T, label=&#39;summarystatisticstable&#39;, format.args = list(scientific = FALSE), digits = 3, linesep = &quot;&quot;) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;), full_width = F) %&gt;% readr::write_lines(&#39;./paper/summarystatisticstable.tex&#39;) Breaking down in all individual datasets if you want to justify the many outliers due to cifar p1&lt;- d %&gt;% dplyr::filter(DataType==&#39;numeric&#39;) %&gt;% ggplot(aes(x=Model, y=Value, fill=Model))+ geom_boxplot()+ theme(axis.text.x = element_blank())+ #remove the x labels facet_wrap(~Dataset)+ labs(title = &#39;Numeric&#39;) p2&lt;- d %&gt;% dplyr::filter(DataType==&#39;image&#39;) %&gt;% ggplot(aes(x=Model, y=Value, fill=Model))+ geom_boxplot()+ theme(axis.text.x = element_blank())+ #remove the x labels facet_wrap(~Dataset)+ labs(title = &#39;Image&#39;) p3&lt;- d %&gt;% dplyr::filter(DataType==&#39;text&#39;) %&gt;% ggplot(aes(x=Model, y=Value, fill=Model))+ geom_boxplot()+ theme(axis.text.x = element_blank())+ #remove the x labels facet_wrap(~Dataset)+ labs(title = &#39;Image&#39;) p1 p2 p3 save_fig(p1,&#39;boxplotsperdataset-numeric.pdf&#39;) save_fig(p2,&#39;boxplotsperdataset-image.pdf&#39;) save_fig(p3,&#39;boxplotsperdataset-text.pdf&#39;) 1.2 Bradley terry model for ranking To create a Bradley terry model we need first to transform our dataset to paired comparisons On each iteration for each dataset for each variable we will rank the models based on the Value of the accuracy (lower accuracy -&gt; smaller) After we expand it to wide so we can compare each algorithm with each other and create a BT dataset d_acc_rank &lt;- d %&gt;% dplyr::group_by(Dataset, Variable, Iteration) %&gt;% dplyr::mutate(Rank=-rank(Value, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-Value) %&gt;% #we need to drop the Value variable to pivot wider tidyr::pivot_wider(names_from = Model, values_from=Rank) Now we can create the BT dataset #a vector with the name of the algorithms models &lt;- get_index_names_as_array(d$Model) n_models = length(models) comb &lt;- gtools::combinations(n=n_models, r=2, v=seq(1:n_models), repeats.allowed = F) #all teh paired combinations d_acc_bt &lt;- dplyr::tribble(~model0_name, ~model0, ~model1_name, ~model1, ~y, ~Iteration, ~Dataset, ~DatasetType) #now we loop each row of the rank wide dataset and create a new one for(i in 1:nrow(d_acc_rank)) { current_row &lt;- d_acc_rank[i,] for(j in 1:nrow(comb)){ comb_row &lt;- comb[j,] model0_name &lt;- models[comb_row[1]] model0 &lt;- comb_row[1] model0_rank &lt;- current_row[[1,model0_name]] model1_name &lt;- models[comb_row[2]] model1 &lt;- comb_row[2] model1_rank &lt;- current_row[[1,model1_name]] diff_rank &lt;- model1_rank - model0_rank #SInce higher accuracy is better if model 1 rank- model 0 rank is positive than model1 wins and y=1 else y=0 y &lt;- ifelse(diff_rank&gt;0, 1, 0) d_acc_bt &lt;-d_acc_bt %&gt;% add_row(model0_name=model0_name, model0=model0, model1_name=model1_name, model1=model1, y=y, Iteration=current_row$Iteration, Dataset=current_row$Dataset, DatasetType=current_row$DataType) } } Now that we have the dataset we can run the model print_stan_code(&#39;./models/rankingmodel.stan&#39;) // Ranking model // Author: David Issa Mattos // Date: 6 sept 2020 // // data { int &lt;lower=1&gt; N_total; // Sample size int y[N_total]; //variable that indicates which one wins model 0 or model 1 int &lt;lower=1&gt; N_models; // Number of models int &lt;lower=1&gt; model0[N_total]; int &lt;lower=1&gt; model1[N_total]; // //To model the influence of each benchmark // int &lt;lower=1&gt; N_bm; // int bm_id[N_total]; } parameters { real a_model[N_models]; //Latent variable that represents the strength value of each model } model { real p[N_total]; a_model ~ normal(0,2); for (i in 1:N_total) { p[i] = a_model[model0[i]] - a_model[model1[i]]; } y ~ bernoulli_logit(p); } //Uncoment this part to get the posterior predictives and the log likelihood //But note that it takes a lot of space in the final model // generated quantities{ // vecor [N_total] y_rep; // vector[N_total] log_lik; // for(i in 1:N_total){ // real p; // p = a_alg[algo1[i]] - a_alg[algo0[i]]; // y_rep[i] = bernoulli_logit_rng(p); // // //Log likelihood // log_lik[i] = bernoulli_logit_lpmf(y[i] | p); // } // } m1_data &lt;- list( N_total=nrow(d_acc_bt), y = as.integer(d_acc_bt$y), N_models = as.integer(n_models), model0=as.integer(d_acc_bt$model0), model1=as.integer(d_acc_bt$model1) ) m1_fit &lt;- stan(file = &#39;./models/rankingmodel.stan&#39;, data=m1_data, chains = 4, warmup = 200, iter = 2000) saveRDS(m1_fit, file = &quot;./data/m1_fit.RDS&quot;) m1_fit &lt;-readRDS(&quot;./data/m1_fit.RDS&quot;) a_model &lt;- c(&quot;a_model[1]&quot;, &quot;a_model[2]&quot;, &quot;a_model[3]&quot;, &quot;a_model[4]&quot;, &quot;a_model[5]&quot;, &quot;a_model[6]&quot;, &quot;a_model[7]&quot;) rstan::traceplot(m1_fit, pars=a_model) hpdi &lt;- get_HPDI_from_stanfit(m1_fit) hpdi_algorithm &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_model\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=models) p&lt;-ggplot(data=hpdi_algorithm, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;Estimate&quot;, x=&quot;Model&quot;, title = &quot;HPDI of the strength of the model&quot;)+ coord_flip() p save_fig(p,&#39;strength-aggregated.pdf&#39;) Here we are extracting all samples and ranking them to have a distribution of the ranks posterior &lt;- rstan::extract(m1_fit) a_model &lt;- as_tibble(posterior$a_model) colnames(a_model) &lt;- models #sampling from the posterior s &lt;- dplyr::sample_n(a_model, size = 1000, replace=T) s &lt;- dplyr::mutate(s, rown = row_number()) wide_s &lt;- tidyr::pivot_longer(s, cols=all_of(models), names_to = &quot;Models&quot;, values_to = &quot;a_model&quot;) rank_df &lt;- wide_s %&gt;% dplyr::group_by(rown) %&gt;% dplyr::mutate(Rank = rank(-a_model, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-a_model) %&gt;% dplyr::group_by(Models) %&gt;% dplyr::summarise(MedianRank = median(Rank), VarianceRank = var(Rank)) %&gt;% dplyr::arrange(MedianRank) rank_df_table &lt;- rank_df colnames(rank_df_table) &lt;- c(&quot;Models&quot;,&quot;Median Rank&quot;, &quot;Variance of the Rank&quot;) kable(rank_df_table, &quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) Models Median Rank Variance of the Rank Active_Uncertainty 1 0.1856697 Active_QBC 2 0.2194905 Active_Random 3 0.0224935 LabelSpreading_rbf 4 0.2922673 LabelPropagation_knn 5 0.3933373 LabelSpreading_knn 6 0.2183774 LabelPropagation_rbf 7 0.0000000 saving the table to latex rank_df_table %&gt;% kable( &quot;latex&quot;, table.envir = &#39;table&#39;, caption=&#39;Ranking of the algorithms&#39;, booktabs=T, label=&#39;rankingaggregated&#39;, format.args = list(scientific = FALSE), digits = 3, linesep = &quot;&quot;) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;), full_width = F) %&gt;% readr::write_lines(&#39;./paper/rankingaggregated.tex&#39;) "],["rq2.html", "Chapter 2 RQ2 2.1 Descriptive statistics 2.2 Bradley terry model for ranking 2.3 Results for 10% 2.4 Results for 50%", " Chapter 2 RQ2 How do the rank of these algorithms change with changes in the amount of manual label effort prior to applying these methods? d &lt;- read_csv(&#39;./data/auto-label-comparison.csv&#39;) %&gt;% dplyr::rename(Value=&#39;value&#39;) %&gt;% dplyr::filter(ValueType==&#39;accuracy&#39;) d10 &lt;- dplyr::filter(d,Variable==&#39;10%&#39;) d50 &lt;- dplyr::filter(d,Variable==&#39;50%&#39;) 2.1 Descriptive statistics p&lt;-ggplot(data=d, aes(x=Model, y=Value, fill=Model))+ geom_boxplot()+ theme(axis.text.x = element_blank())+ #remove the x labels facet_wrap(~Variable)+ labs(title = &#39;Accuracy (all datasets)&#39;) p save_fig(p,&#39;aggregatedmanualeffortboxplots.pdf&#39;) A respective table for this box-plot but with 5% and 95% quantiles d %&gt;% dplyr::group_by(Variable,Model) %&gt;% summarise(Mean = mean(Value), SD = sd(Value), Median = median(Value), &#39;5%&#39; = quantile(Value,0.05), &#39;95%&#39; = quantile(Value,0.95)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-Variable) %&gt;% kable( &quot;latex&quot;, table.envir = &#39;table&#39;, caption=&#39;Summary statistics for the accuracy aggregated data&#39;, booktabs=T, label=&#39;summarystatisticstable&#39;, format.args = list(scientific = FALSE), digits = 3, linesep = &quot;&quot;) %&gt;% pack_rows(&quot;10% labels available&quot;,1, n_models) %&gt;% pack_rows(&quot;50% labels available&quot;,1+n_models,2*n_models) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;), full_width = F) %&gt;% readr::write_lines(&#39;./paper/summarystatisticstablemanualeffort.tex&#39;) 2.2 Bradley terry model for ranking To create a Bradley terry model we need first to transform our dataset to paired comparisons On each iteration for each dataset for each variable we will rank the models based on the Value of the accuracy (lower accuracy -&gt; smaller) After we expand it to wide so we can compare each algorithm with each other and create a BT dataset d_acc_rank_10 &lt;- d10 %&gt;% dplyr::group_by(Dataset, Variable, Iteration) %&gt;% dplyr::mutate(Rank=-rank(Value, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-Value) %&gt;% #we need to drop the Value variable to pivot wider tidyr::pivot_wider(names_from = Model, values_from=Rank) d_acc_rank_50 &lt;- d50 %&gt;% dplyr::group_by(Dataset, Variable, Iteration) %&gt;% dplyr::mutate(Rank=-rank(Value, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-Value) %&gt;% #we need to drop the Value variable to pivot wider tidyr::pivot_wider(names_from = Model, values_from=Rank) Now we can create the BT dataset #a vector with the name of the algorithms models &lt;- get_index_names_as_array(d$Model) n_models = length(models) comb &lt;- gtools::combinations(n=n_models, r=2, v=seq(1:n_models), repeats.allowed = F) #all teh paired combinations d_acc_bt_10 &lt;- dplyr::tribble(~model0_name, ~model0, ~model1_name, ~model1, ~y, ~Iteration, ~Dataset, ~DataType) d_acc_bt_50 &lt;- dplyr::tribble(~model0_name, ~model0, ~model1_name, ~model1, ~y, ~Iteration, ~Dataset, ~DataType) #now we loop each row of the rank wide dataset and create a new one for(i in 1:nrow(d_acc_rank_10)) { current_row &lt;- d_acc_rank_10[i,] for(j in 1:nrow(comb)){ comb_row &lt;- comb[j,] model0_name &lt;- models[comb_row[1]] model0 &lt;- comb_row[1] model0_rank &lt;- current_row[[1,model0_name]] model1_name &lt;- models[comb_row[2]] model1 &lt;- comb_row[2] model1_rank &lt;- current_row[[1,model1_name]] diff_rank &lt;- model1_rank - model0_rank #SInce higher accuracy is better if model 1 rank- model 0 rank is positive than model1 wins and y=1 else y=0 y &lt;- ifelse(diff_rank&gt;0, 1, 0) d_acc_bt_10 &lt;-d_acc_bt_10 %&gt;% add_row(model0_name=model0_name, model0=model0, model1_name=model1_name, model1=model1, y=y, Iteration=current_row$Iteration, Dataset=current_row$Dataset, DataType=current_row$DataType) } } #now we loop each row of the rank wide dataset and create a new one for(i in 1:nrow(d_acc_rank_50)) { current_row &lt;- d_acc_rank_50[i,] for(j in 1:nrow(comb)){ comb_row &lt;- comb[j,] model0_name &lt;- models[comb_row[1]] model0 &lt;- comb_row[1] model0_rank &lt;- current_row[[1,model0_name]] model1_name &lt;- models[comb_row[2]] model1 &lt;- comb_row[2] model1_rank &lt;- current_row[[1,model1_name]] diff_rank &lt;- model1_rank - model0_rank #SInce higher accuracy is better if model 1 rank- model 0 rank is positive than model1 wins and y=1 else y=0 y &lt;- ifelse(diff_rank&gt;0, 1, 0) d_acc_bt_50 &lt;-d_acc_bt_50 %&gt;% add_row(model0_name=model0_name, model0=model0, model1_name=model1_name, model1=model1, y=y, Iteration=current_row$Iteration, Dataset=current_row$Dataset, DataType=current_row$DataType) } } m10_data &lt;- list( N_total=nrow(d_acc_bt_10), y = as.integer(d_acc_bt_10$y), N_models = as.integer(n_models), model0=as.integer(d_acc_bt_10$model0), model1=as.integer(d_acc_bt_10$model1) ) m50_data &lt;- list( N_total=nrow(d_acc_bt_50), y = as.integer(d_acc_bt_50$y), N_models = as.integer(n_models), model0=as.integer(d_acc_bt_50$model0), model1=as.integer(d_acc_bt_50$model1) ) we use the same model but with different data m10_fit &lt;- stan(file = &#39;./models/rankingmodel.stan&#39;, data=m10_data, chains = 4, warmup = 200, iter = 2000) saveRDS(m10_fit, file = &quot;./data/m10_fit.RDS&quot;) m50_fit &lt;- stan(file = &#39;./models/rankingmodel.stan&#39;, data=m50_data, chains = 4, warmup = 200, iter = 2000) saveRDS(m50_fit, file = &quot;./data/m50_fit.RDS&quot;) 2.3 Results for 10% m10_fit &lt;-readRDS(&quot;./data/m10_fit.RDS&quot;) a_model &lt;- c(&quot;a_model[1]&quot;, &quot;a_model[2]&quot;, &quot;a_model[3]&quot;, &quot;a_model[4]&quot;, &quot;a_model[5]&quot;, &quot;a_model[6]&quot;, &quot;a_model[7]&quot;) rstan::traceplot(m10_fit, pars=a_model) hpdi &lt;- get_HPDI_from_stanfit(m10_fit) hpdi_algorithm &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_model\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=models) p&lt;-ggplot(data=hpdi_algorithm, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;Estimate&quot;, x=&quot;Model&quot;, title = &quot;HPDI of the strength of the model with 10% of available labels&quot;)+ coord_flip() p save_fig(p,&#39;strength-10.pdf&#39;) Here we are extracting all samples and ranking them to have a distribution of the ranks posterior &lt;- rstan::extract(m10_fit) a_model &lt;- as_tibble(posterior$a_model) colnames(a_model) &lt;- models #sampling from the posterior s &lt;- dplyr::sample_n(a_model, size = 1000, replace=T) s &lt;- dplyr::mutate(s, rown = row_number()) wide_s &lt;- tidyr::pivot_longer(s, cols=all_of(models), names_to = &quot;Models&quot;, values_to = &quot;a_model&quot;) rank_df &lt;- wide_s %&gt;% dplyr::group_by(rown) %&gt;% dplyr::mutate(Rank = rank(-a_model, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-a_model) %&gt;% dplyr::group_by(Models) %&gt;% dplyr::summarise(MedianRank = median(Rank), VarianceRank = var(Rank)) %&gt;% dplyr::arrange(MedianRank) rank_df_table &lt;- rank_df colnames(rank_df_table) &lt;- c(&quot;Models&quot;,&quot;Median Rank&quot;, &quot;Variance of the Rank&quot;) kable(rank_df_table, &quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) Models Median Rank Variance of the Rank Active_Uncertainty 1 0.1154264 Active_QBC 2 0.1229940 Active_Random 3 0.0059700 LabelPropagation_knn 5 0.6920160 LabelSpreading_knn 5 0.6786537 LabelSpreading_rbf 5 0.6267858 LabelPropagation_rbf 7 0.0000000 saving the table to latex rank_df_table %&gt;% kable( &quot;latex&quot;, table.envir = &#39;table&#39;, caption=&#39;Ranking of the algorithms with 10% of available labels&#39;, booktabs=T, label=&#39;ranking10&#39;, format.args = list(scientific = FALSE), digits = 3, linesep = &quot;&quot;) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;), full_width = F) %&gt;% readr::write_lines(&#39;./paper/ranking10.tex&#39;) 2.4 Results for 50% m50_fit &lt;-readRDS(&quot;./data/m50_fit.RDS&quot;) a_model &lt;- c(&quot;a_model[1]&quot;, &quot;a_model[2]&quot;, &quot;a_model[3]&quot;, &quot;a_model[4]&quot;, &quot;a_model[5]&quot;, &quot;a_model[6]&quot;, &quot;a_model[7]&quot;) rstan::traceplot(m50_fit, pars=a_model) hpdi &lt;- get_HPDI_from_stanfit(m50_fit) hpdi_algorithm &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_model\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=models) p&lt;-ggplot(data=hpdi_algorithm, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;Estimate&quot;, x=&quot;Model&quot;, title = &quot;HPDI of the strength of the model with 50% of available labels&quot;)+ coord_flip() p save_fig(p,&#39;strength-50.pdf&#39;) Here we are extracting all samples and ranking them to have a distribution of the ranks posterior &lt;- rstan::extract(m50_fit) a_model &lt;- as_tibble(posterior$a_model) colnames(a_model) &lt;- models #sampling from the posterior s &lt;- dplyr::sample_n(a_model, size = 1000, replace=T) s &lt;- dplyr::mutate(s, rown = row_number()) wide_s &lt;- tidyr::pivot_longer(s, cols=all_of(models), names_to = &quot;Models&quot;, values_to = &quot;a_model&quot;) rank_df &lt;- wide_s %&gt;% dplyr::group_by(rown) %&gt;% dplyr::mutate(Rank = rank(-a_model, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-a_model) %&gt;% dplyr::group_by(Models) %&gt;% dplyr::summarise(MedianRank = median(Rank), VarianceRank = var(Rank)) %&gt;% dplyr::arrange(MedianRank) rank_df_table &lt;- rank_df colnames(rank_df_table) &lt;- c(&quot;Models&quot;,&quot;Median Rank&quot;, &quot;Variance of the Rank&quot;) kable(rank_df_table, &quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) Models Median Rank Variance of the Rank Active_QBC 2 0.5785776 Active_Uncertainty 2 0.6213854 Active_Random 3 0.6001512 LabelPropagation_knn 5 0.6578418 LabelSpreading_knn 5 0.6270581 LabelSpreading_rbf 5 0.6259820 LabelPropagation_rbf 7 0.0049800 saving the table to latex rank_df_table %&gt;% kable( &quot;latex&quot;, table.envir = &#39;table&#39;, caption=&#39;Ranking of the algorithms with 50% of available labels&#39;, booktabs=T, label=&#39;ranking10&#39;, format.args = list(scientific = FALSE), digits = 3, linesep = &quot;&quot;) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;), full_width = F) %&gt;% readr::write_lines(&#39;./paper/ranking50.tex&#39;) "]]
