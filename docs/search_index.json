[["index.html", "Automatic labeling comparison - Online appendix Preface", " Automatic labeling comparison - Online appendix Teodor Fredriksson, David Issa Mattos, Jan Bosch, Helena Holmstr√∂m Olsson 27 May, 2021 Preface Packages used for the analysis library(knitr) library(rmdformats) library(kableExtra) # #data processing library(tidyverse) library(glue) # #modeling library(cmdstanr) library(posterior) library(bayesplot) # #plotting library(patchwork) library(viridis) library(ggthemr)#devtools::install_github(&#39;cttobin/ggthemr&#39;) ggthemr(&#39;flat&#39;) library(progress) library(gtools) #sourcing local files source(&quot;utils.R&quot;) knitr::opts_chunk$set( echo=T, warning=FALSE, include=T, cache=T, prompt=FALSE, tidy=FALSE, comment=NA, message=FALSE, fig.align=&#39;center&#39;) knitr::opts_knit$set(width=75) source(&quot;utils.R&quot;) This document was created with the bookdown package. To compile it (and run every command to generate the models, figures and etc. ) use the custom function from the utils.R file. compile_book() This document was compiled under the following session sessionInfo() R version 4.1.0 (2021-05-18) Platform: x86_64-apple-darwin17.0 (64-bit) Running under: macOS Big Sur 10.16 Matrix products: default BLAS: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.dylib LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib locale: [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 attached base packages: [1] stats graphics grDevices utils datasets methods base other attached packages: [1] coda_0.19-4 rstan_2.21.2 StanHeaders_2.21.0-7 [4] gtools_3.8.2 progress_1.2.2 ggthemr_1.1.0 [7] viridis_0.6.1 viridisLite_0.4.0 patchwork_1.1.1 [10] bayesplot_1.8.0 posterior_0.1.6 cmdstanr_0.4.0.9000 [13] glue_1.4.2 forcats_0.5.1 stringr_1.4.0 [16] dplyr_1.0.6 purrr_0.3.4 readr_1.4.0 [19] tidyr_1.1.3 tibble_3.1.2 ggplot2_3.3.3 [22] tidyverse_1.3.1 kableExtra_1.3.4 rmdformats_1.0.2 [25] knitr_1.33 loaded via a namespace (and not attached): [1] matrixStats_0.58.0 fs_1.5.0 lubridate_1.7.10 [4] webshot_0.5.2 httr_1.4.2 tensorA_0.36.2 [7] tools_4.1.0 backports_1.2.1 bslib_0.2.5.1 [10] utf8_1.2.1 R6_2.5.0 DBI_1.1.1 [13] colorspace_2.0-1 withr_2.4.2 processx_3.5.2 [16] tidyselect_1.1.1 gridExtra_2.3 prettyunits_1.1.1 [19] curl_4.3.1 compiler_4.1.0 cli_2.5.0 [22] rvest_1.0.0 xml2_1.3.2 bookdown_0.22 [25] sass_0.4.0 scales_1.1.1 checkmate_2.0.0 [28] ggridges_0.5.3 callr_3.7.0 systemfonts_1.0.2 [31] digest_0.6.27 rmarkdown_2.8 svglite_2.0.0 [34] pkgconfig_2.0.3 htmltools_0.5.1.1 dbplyr_2.1.1 [37] rlang_0.4.11 readxl_1.3.1 rstudioapi_0.13 [40] jquerylib_0.1.4 farver_2.1.0 generics_0.1.0 [43] jsonlite_1.7.2 distributional_0.2.2 inline_0.3.18 [46] magrittr_2.0.1 loo_2.4.1 Rcpp_1.0.6 [49] munsell_0.5.0 fansi_0.5.0 abind_1.4-5 [52] lifecycle_1.0.0 stringi_1.6.2 yaml_2.2.1 [55] pkgbuild_1.2.0 plyr_1.8.6 grid_4.1.0 [58] parallel_4.1.0 crayon_1.4.1 lattice_0.20-44 [61] haven_2.4.1 hms_1.1.0 ps_1.6.0 [64] pillar_1.6.1 codetools_0.2-18 stats4_4.1.0 [67] reprex_2.0.0 evaluate_0.14 V8_3.4.2 [70] RcppParallel_5.1.4 modelr_0.1.8 vctrs_0.3.8 [73] cellranger_1.1.0 gtable_0.3.0 assertthat_0.2.1 [76] xfun_0.23 broom_0.7.6 ellipsis_0.3.2 "],["rq1.html", "Chapter 1 RQ1 1.1 Descriptive statistics 1.2 Bradley terry model for ranking 1.3 Diagnostics 1.4 Results", " Chapter 1 RQ1 How can we rank different active learning and semi-supervised learning algorithms in terms of accuracy? d &lt;- read_csv(&#39;./data/full_data.csv&#39;) %&gt;% dplyr::rename(Value=&#39;value&#39;) %&gt;% select(-X1) # %&gt;% # dplyr::filter(ValueType==&#39;accuracy&#39;) 1.1 Descriptive statistics p&lt;-ggplot(data=d, aes(x=Model, y=Value, fill=Model))+ geom_boxplot()+ theme(axis.text.x = element_blank())+ #remove the x labels labs(title = &#39;Accuracy (all datasets)&#39;) p save_fig(p,&#39;aggregatedboxplots.pdf&#39;) A respective table for this box-plot but with 5% and 95% quantiles d %&gt;% dplyr::group_by(Model) %&gt;% summarise(Mean = mean(Value), SD = sd(Value), Median = median(Value), &#39;5%&#39; = quantile(Value,0.05), &#39;95%&#39; = quantile(Value,0.95)) %&gt;% dplyr::ungroup() %&gt;% kable( &quot;latex&quot;, table.envir = &#39;table&#39;, caption=&#39;Summary statistics for the accuracy aggregated data&#39;, booktabs=T, label=&#39;summarystatisticstable&#39;, format.args = list(scientific = FALSE), digits = 3, linesep = &quot;&quot;) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;), full_width = F) %&gt;% readr::write_lines(&#39;./paper/summarystatisticstable.tex&#39;) Breaking down in all individual datasets if you want to justify the many outliers due to cifar p1&lt;- d %&gt;% dplyr::filter(DataType==&#39;numeric&#39;) %&gt;% ggplot(aes(x=Model, y=Value, fill=Model))+ geom_boxplot()+ theme(axis.text.x = element_blank())+ #remove the x labels facet_wrap(~Dataset)+ labs(title = &#39;Numeric&#39;) p2&lt;- d %&gt;% dplyr::filter(DataType==&#39;image&#39;) %&gt;% ggplot(aes(x=Model, y=Value, fill=Model))+ geom_boxplot()+ theme(axis.text.x = element_blank())+ #remove the x labels facet_wrap(~Dataset)+ labs(title = &#39;Image&#39;) p3&lt;- d %&gt;% dplyr::filter(DataType==&#39;text&#39;) %&gt;% ggplot(aes(x=Model, y=Value, fill=Model))+ geom_boxplot()+ theme(axis.text.x = element_blank())+ #remove the x labels facet_wrap(~Dataset)+ labs(title = &#39;Text&#39;) p1 p2 p3 save_fig(p1,&#39;boxplotsperdataset-numeric.pdf&#39;) save_fig(p2,&#39;boxplotsperdataset-image.pdf&#39;) save_fig(p3,&#39;boxplotsperdataset-text.pdf&#39;) 1.2 Bradley terry model for ranking To create a Bradley terry model we need first to transform our dataset to paired comparisons On each iteration for each dataset for each variable we will rank the models based on the Value of the accuracy (lower accuracy -&gt; smaller) After we expand it to wide so we can compare each algorithm with each other and create a BT dataset d_acc_rank &lt;- d %&gt;% dplyr::group_by(Dataset, Variable, Iteration) %&gt;% dplyr::mutate(Rank=-rank(Value, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-Value) %&gt;% #we need to drop the Value variable to pivot wider tidyr::pivot_wider(names_from = Model, values_from=Rank) Now we can create the BT dataset #a vector with the name of the algorithms models &lt;- get_index_names_as_array(d$Model) n_models = length(models) comb &lt;- gtools::combinations(n=n_models, r=2, v=seq(1:n_models), repeats.allowed = F) #all teh paired combinations d_acc_bt &lt;- dplyr::tribble(~model0_name, ~model0, ~model1_name, ~model1, ~y, ~Iteration, ~Dataset, ~DatasetType) #now we loop each row of the rank wide dataset and create a new one for(i in 1:nrow(d_acc_rank)) { current_row &lt;- d_acc_rank[i,] for(j in 1:nrow(comb)){ comb_row &lt;- comb[j,] model0_name &lt;- models[comb_row[1]] model0 &lt;- comb_row[1] model0_rank &lt;- current_row[[1,model0_name]] model1_name &lt;- models[comb_row[2]] model1 &lt;- comb_row[2] model1_rank &lt;- current_row[[1,model1_name]] diff_rank &lt;- model1_rank - model0_rank #SInce higher accuracy is better if model 1 rank- model 0 rank is positive than model1 wins and y=1 else y=0 y &lt;- ifelse(diff_rank&gt;0, 1, 0) d_acc_bt &lt;-d_acc_bt %&gt;% add_row(model0_name=model0_name, model0=model0, model1_name=model1_name, model1=model1, y=y, Iteration=current_row$Iteration, Dataset=current_row$Dataset, DatasetType=current_row$DataType) } } Now that we have the dataset we can run the model print_stan_code(&#39;./models/rankingmodel.stan&#39;) // Ranking model // Author: David Issa Mattos // Date: 6 sept 2020 // // data { int &lt;lower=1&gt; N_total; // Sample size int y[N_total]; //variable that indicates which one wins model 0 or model 1 int &lt;lower=1&gt; N_models; // Number of models int &lt;lower=1&gt; model0[N_total]; int &lt;lower=1&gt; model1[N_total]; // //To model the influence of each benchmark // int &lt;lower=1&gt; N_bm; // int bm_id[N_total]; } parameters { real a_model[N_models]; //Latent variable that represents the strength value of each model } model { real p[N_total]; a_model ~ normal(0,2); for (i in 1:N_total) { p[i] = a_model[model0[i]] - a_model[model1[i]]; } y ~ bernoulli_logit(p); } //Uncoment this part to get the posterior predictives and the log likelihood //But note that it takes a lot of space in the final model // generated quantities{ // vecor [N_total] y_rep; // vector[N_total] log_lik; // for(i in 1:N_total){ // real p; // p = a_alg[algo1[i]] - a_alg[algo0[i]]; // y_rep[i] = bernoulli_logit_rng(p); // // //Log likelihood // log_lik[i] = bernoulli_logit_lpmf(y[i] | p); // } // } m1_data &lt;- list( N_total=nrow(d_acc_bt), y = as.integer(d_acc_bt$y), N_models = as.integer(n_models), model0=as.integer(d_acc_bt$model0), model1=as.integer(d_acc_bt$model1) ) model &lt;- cmdstanr::cmdstan_model(stan_file = &#39;./models/rankingmodel.stan&#39;) m1_fit &lt;- model$sample( data = m1_data, chains = 4, iter = 2000, iter_warmup = 200, parallel_chains = 4, seed = 3103, ) m1_fit$save_object(file = &quot;./data/m1_fit.RDS&quot;) 1.3 Diagnostics m1_fit &lt;-readRDS(&quot;./data/m1_fit.RDS&quot;) a_model &lt;- c(&quot;a_model[1]&quot;, &quot;a_model[2]&quot;, &quot;a_model[3]&quot;, &quot;a_model[4]&quot;, &quot;a_model[5]&quot;, &quot;a_model[6]&quot;, &quot;a_model[7]&quot;) draws_a &lt;- posterior::as_draws(m1_fit$draws(variables = a_model)) bayesplot::mcmc_trace(draws_a, pars=a_model) 1.4 Results p&lt;-mcmc_intervals(draws_a) + scale_y_discrete(labels=models)+ labs(x=&#39;Estimate&#39;, y=&#39;Model&#39;, title=&#39;Strength parameters&#39;) p save_fig(p,&#39;strength-aggregated.pdf&#39;) Here we are extracting all samples and ranking them to have a distribution of the ranks posterior_df &lt;- as.data.frame(posterior::as_draws_df(m1_fit$draws(variables = a_model)))[,1:n_models] colnames(posterior_df) &lt;- models #sampling from the posterior s &lt;- dplyr::sample_n(posterior_df, size = 1000, replace=T) s &lt;- dplyr::mutate(s, rown = row_number()) wide_s &lt;- tidyr::pivot_longer(s, cols=all_of(models), names_to = &quot;Models&quot;, values_to = &quot;a_model&quot;) rank_df &lt;- wide_s %&gt;% dplyr::group_by(rown) %&gt;% dplyr::mutate(Rank = rank(-a_model, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-a_model) %&gt;% dplyr::group_by(Models) %&gt;% dplyr::summarise(MedianRank = median(Rank), VarianceRank = var(Rank)) %&gt;% dplyr::arrange(MedianRank) rank_df_table &lt;- rank_df colnames(rank_df_table) &lt;- c(&quot;Models&quot;,&quot;Median Rank&quot;, &quot;Variance of the Rank&quot;) kable(rank_df_table, &quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) Models Median Rank Variance of the Rank Active_uncertainty 1 0.0216216 Active_QBC 2 0.2366356 Active_random 3 0.2301892 LabelPropagation_rbf 4 0.0176937 LabelSpreading_knn 5 0.1913273 LabelSpreading_rbf 6 0.1660661 LabelPropagation_knn 7 0.0000000 saving the table to latex rank_df_table %&gt;% kable( &quot;latex&quot;, table.envir = &#39;table&#39;, caption=&#39;Ranking of the algorithms&#39;, booktabs=T, label=&#39;rankingaggregated&#39;, format.args = list(scientific = FALSE), digits = 3, linesep = &quot;&quot;) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;), full_width = F) %&gt;% readr::write_lines(&#39;./paper/rankingaggregated.tex&#39;) "],["rq2.html", "Chapter 2 RQ2 2.1 Descriptive statistics 2.2 Bradley terry model for ranking 2.3 Results for 10% 2.4 Results for 50%", " Chapter 2 RQ2 How do the rank of these algorithms change with changes in the amount of manual label effort prior to applying these methods? d &lt;- read_csv(&#39;./data/full_data.csv&#39;) %&gt;% dplyr::rename(Value=&#39;value&#39;) %&gt;% select(-X1) d10 &lt;- dplyr::filter(d,Variable==&#39;10%&#39;) d50 &lt;- dplyr::filter(d,Variable==&#39;50%&#39;) 2.1 Descriptive statistics p&lt;-ggplot(data=d, aes(x=Model, y=Value, fill=Model))+ geom_boxplot()+ theme(axis.text.x = element_blank())+ #remove the x labels facet_wrap(~Variable)+ labs(title = &#39;Accuracy (all datasets)&#39;) p save_fig(p,&#39;aggregatedmanualeffortboxplots.pdf&#39;) A respective table for this box-plot but with 5% and 95% quantiles d %&gt;% dplyr::group_by(Variable,Model) %&gt;% summarise(Mean = mean(Value), SD = sd(Value), Median = median(Value), &#39;5%&#39; = quantile(Value,0.05), &#39;95%&#39; = quantile(Value,0.95)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-Variable) %&gt;% kable( &quot;latex&quot;, table.envir = &#39;table&#39;, caption=&#39;Summary statistics for the accuracy aggregated data&#39;, booktabs=T, label=&#39;summarystatisticstable&#39;, format.args = list(scientific = FALSE), digits = 3, linesep = &quot;&quot;) %&gt;% pack_rows(&quot;10% labels available&quot;,1, n_models) %&gt;% pack_rows(&quot;50% labels available&quot;,1+n_models,2*n_models) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;), full_width = F) %&gt;% readr::write_lines(&#39;./paper/summarystatisticstablemanualeffort.tex&#39;) 2.2 Bradley terry model for ranking To create a Bradley terry model we need first to transform our dataset to paired comparisons On each iteration for each dataset for each variable we will rank the models based on the Value of the accuracy (lower accuracy -&gt; smaller) After we expand it to wide so we can compare each algorithm with each other and create a BT dataset d_acc_rank_10 &lt;- d10 %&gt;% dplyr::group_by(Dataset, Variable, Iteration) %&gt;% dplyr::mutate(Rank=-rank(Value, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-Value) %&gt;% #we need to drop the Value variable to pivot wider tidyr::pivot_wider(names_from = Model, values_from=Rank) d_acc_rank_50 &lt;- d50 %&gt;% dplyr::group_by(Dataset, Variable, Iteration) %&gt;% dplyr::mutate(Rank=-rank(Value, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-Value) %&gt;% #we need to drop the Value variable to pivot wider tidyr::pivot_wider(names_from = Model, values_from=Rank) Now we can create the BT dataset #a vector with the name of the algorithms models &lt;- get_index_names_as_array(d$Model) n_models = length(models) comb &lt;- gtools::combinations(n=n_models, r=2, v=seq(1:n_models), repeats.allowed = F) #all teh paired combinations d_acc_bt_10 &lt;- dplyr::tribble(~model0_name, ~model0, ~model1_name, ~model1, ~y, ~Iteration, ~Dataset, ~DataType) d_acc_bt_50 &lt;- dplyr::tribble(~model0_name, ~model0, ~model1_name, ~model1, ~y, ~Iteration, ~Dataset, ~DataType) #now we loop each row of the rank wide dataset and create a new one for(i in 1:nrow(d_acc_rank_10)) { current_row &lt;- d_acc_rank_10[i,] for(j in 1:nrow(comb)){ comb_row &lt;- comb[j,] model0_name &lt;- models[comb_row[1]] model0 &lt;- comb_row[1] model0_rank &lt;- current_row[[1,model0_name]] model1_name &lt;- models[comb_row[2]] model1 &lt;- comb_row[2] model1_rank &lt;- current_row[[1,model1_name]] diff_rank &lt;- model1_rank - model0_rank #SInce higher accuracy is better if model 1 rank- model 0 rank is positive than model1 wins and y=1 else y=0 y &lt;- ifelse(diff_rank&gt;0, 1, 0) d_acc_bt_10 &lt;-d_acc_bt_10 %&gt;% add_row(model0_name=model0_name, model0=model0, model1_name=model1_name, model1=model1, y=y, Iteration=current_row$Iteration, Dataset=current_row$Dataset, DataType=current_row$DataType) } } #now we loop each row of the rank wide dataset and create a new one for(i in 1:nrow(d_acc_rank_50)) { current_row &lt;- d_acc_rank_50[i,] for(j in 1:nrow(comb)){ comb_row &lt;- comb[j,] model0_name &lt;- models[comb_row[1]] model0 &lt;- comb_row[1] model0_rank &lt;- current_row[[1,model0_name]] model1_name &lt;- models[comb_row[2]] model1 &lt;- comb_row[2] model1_rank &lt;- current_row[[1,model1_name]] diff_rank &lt;- model1_rank - model0_rank #SInce higher accuracy is better if model 1 rank- model 0 rank is positive than model1 wins and y=1 else y=0 y &lt;- ifelse(diff_rank&gt;0, 1, 0) d_acc_bt_50 &lt;-d_acc_bt_50 %&gt;% add_row(model0_name=model0_name, model0=model0, model1_name=model1_name, model1=model1, y=y, Iteration=current_row$Iteration, Dataset=current_row$Dataset, DataType=current_row$DataType) } } m10_data &lt;- list( N_total=nrow(d_acc_bt_10), y = as.integer(d_acc_bt_10$y), N_models = as.integer(n_models), model0=as.integer(d_acc_bt_10$model0), model1=as.integer(d_acc_bt_10$model1) ) m50_data &lt;- list( N_total=nrow(d_acc_bt_50), y = as.integer(d_acc_bt_50$y), N_models = as.integer(n_models), model0=as.integer(d_acc_bt_50$model0), model1=as.integer(d_acc_bt_50$model1) ) we use the same model but with different data model &lt;- cmdstanr::cmdstan_model(stan_file = &#39;./models/rankingmodel.stan&#39;) m10_fit &lt;- model$sample( data = m10_data, chains = 4, iter = 2000, iter_warmup = 200, parallel_chains = 4, seed = 3103, ) m50_fit &lt;- model$sample( data = m50_data, chains = 4, iter = 2000, iter_warmup = 200, parallel_chains = 4, seed = 3103, ) m10_fit$save_object(file = &quot;./data/m10_fit.RDS&quot;) m50_fit$save_object(file = &quot;./data/m50_fit.RDS&quot;) 2.3 Results for 10% m10_fit &lt;-readRDS(&quot;./data/m10_fit.RDS&quot;) a_model &lt;- c(&quot;a_model[1]&quot;, &quot;a_model[2]&quot;, &quot;a_model[3]&quot;, &quot;a_model[4]&quot;, &quot;a_model[5]&quot;, &quot;a_model[6]&quot;, &quot;a_model[7]&quot;) draws_a10 &lt;- posterior::as_draws(m10_fit$draws(variables = a_model)) Basic diagnostics bayesplot::mcmc_trace(draws_a10, pars=a_model) m10_fit$cmdstan_diagnose() Processing csv files: /var/folders/mc/9k42tql17jdbmkzpdp5bgwph0000gp/T/RtmpsiNBkA/rankingmodel-202105270844-1-063998.csv, /var/folders/mc/9k42tql17jdbmkzpdp5bgwph0000gp/T/RtmpsiNBkA/rankingmodel-202105270844-2-063998.csv, /var/folders/mc/9k42tql17jdbmkzpdp5bgwph0000gp/T/RtmpsiNBkA/rankingmodel-202105270844-3-063998.csv, /var/folders/mc/9k42tql17jdbmkzpdp5bgwph0000gp/T/RtmpsiNBkA/rankingmodel-202105270844-4-063998.csv Checking sampler transitions treedepth. Treedepth satisfactory for all transitions. Checking sampler transitions for divergences. No divergent transitions found. Checking E-BFMI - sampler transitions HMC potential energy. E-BFMI satisfactory for all transitions. Effective sample size satisfactory. Split R-hat values satisfactory all parameters. Processing complete, no problems detected. p&lt;-mcmc_intervals(draws_a10) + scale_y_discrete(labels=models)+ labs(x=&#39;Estimate&#39;, y=&#39;Model&#39;, title=&#39;Strength parameters - Condition: 10%)&#39;) p save_fig(p,&#39;strength-10.pdf&#39;) Here we are extracting all samples and ranking them to have a distribution of the ranks posterior_df &lt;- as.data.frame(posterior::as_draws_df(m10_fit$draws(variables = a_model)))[,1:n_models] colnames(posterior_df) &lt;- models #sampling from the posterior s &lt;- dplyr::sample_n(posterior_df, size = 1000, replace=T) s &lt;- dplyr::mutate(s, rown = row_number()) wide_s &lt;- tidyr::pivot_longer(s, cols=all_of(models), names_to = &quot;Models&quot;, values_to = &quot;a_model&quot;) rank_df &lt;- wide_s %&gt;% dplyr::group_by(rown) %&gt;% dplyr::mutate(Rank = rank(-a_model, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-a_model) %&gt;% dplyr::group_by(Models) %&gt;% dplyr::summarise(MedianRank = median(Rank), VarianceRank = var(Rank)) %&gt;% dplyr::arrange(MedianRank) rank_df_table &lt;- rank_df colnames(rank_df_table) &lt;- c(&quot;Models&quot;,&quot;Median Rank&quot;, &quot;Variance of the Rank&quot;) kable(rank_df_table, &quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) Models Median Rank Variance of the Rank Active_uncertainty 1 0.0546537 Active_QBC 2 0.2213253 Active_random 3 0.1738048 LabelPropagation_rbf 4 0.1154264 LabelSpreading_knn 5 0.1242482 LabelSpreading_rbf 6 0.0258819 LabelPropagation_knn 7 0.0186577 Saving the table to latex rank_df_table %&gt;% kable( &quot;latex&quot;, table.envir = &#39;table&#39;, caption=&#39;Ranking of the algorithms with 10% of available labels&#39;, booktabs=T, label=&#39;ranking10&#39;, format.args = list(scientific = FALSE), digits = 3, linesep = &quot;&quot;) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;), full_width = F) %&gt;% readr::write_lines(&#39;./paper/ranking10.tex&#39;) 2.4 Results for 50% m50_fit &lt;-readRDS(&quot;./data/m50_fit.RDS&quot;) a_model &lt;- c(&quot;a_model[1]&quot;, &quot;a_model[2]&quot;, &quot;a_model[3]&quot;, &quot;a_model[4]&quot;, &quot;a_model[5]&quot;, &quot;a_model[6]&quot;, &quot;a_model[7]&quot;) draws_a50 &lt;- posterior::as_draws(m50_fit$draws(variables = a_model)) Basic diagnostics bayesplot::mcmc_trace(draws_a50, pars=a_model) m50_fit$cmdstan_diagnose() Processing csv files: /var/folders/mc/9k42tql17jdbmkzpdp5bgwph0000gp/T/RtmpsiNBkA/rankingmodel-202105270835-1-4e8263.csv, /var/folders/mc/9k42tql17jdbmkzpdp5bgwph0000gp/T/RtmpsiNBkA/rankingmodel-202105270835-2-4e8263.csv, /var/folders/mc/9k42tql17jdbmkzpdp5bgwph0000gp/T/RtmpsiNBkA/rankingmodel-202105270835-3-4e8263.csv, /var/folders/mc/9k42tql17jdbmkzpdp5bgwph0000gp/T/RtmpsiNBkA/rankingmodel-202105270835-4-4e8263.csv Checking sampler transitions treedepth. Treedepth satisfactory for all transitions. Checking sampler transitions for divergences. No divergent transitions found. Checking E-BFMI - sampler transitions HMC potential energy. E-BFMI satisfactory for all transitions. Effective sample size satisfactory. Split R-hat values satisfactory all parameters. Processing complete, no problems detected. p&lt;-mcmc_intervals(draws_a50) + scale_y_discrete(labels=models)+ labs(x=&#39;Estimate&#39;, y=&#39;Model&#39;, title=&#39;Strength parameters - Condition: 50%)&#39;) p save_fig(p,&#39;strength-50.pdf&#39;) Here we are extracting all samples and ranking them to have a distribution of the ranks posterior_df &lt;- as.data.frame(posterior::as_draws_df(m50_fit$draws(variables = a_model)))[,1:n_models] colnames(posterior_df) &lt;- models #sampling from the posterior s &lt;- dplyr::sample_n(posterior_df, size = 1000, replace=T) s &lt;- dplyr::mutate(s, rown = row_number()) wide_s &lt;- tidyr::pivot_longer(s, cols=all_of(models), names_to = &quot;Models&quot;, values_to = &quot;a_model&quot;) rank_df &lt;- wide_s %&gt;% dplyr::group_by(rown) %&gt;% dplyr::mutate(Rank = rank(-a_model, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-a_model) %&gt;% dplyr::group_by(Models) %&gt;% dplyr::summarise(MedianRank = median(Rank), VarianceRank = var(Rank)) %&gt;% dplyr::arrange(MedianRank) rank_df_table &lt;- rank_df colnames(rank_df_table) &lt;- c(&quot;Models&quot;,&quot;Median Rank&quot;, &quot;Variance of the Rank&quot;) kable(rank_df_table, &quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) Models Median Rank Variance of the Rank Active_uncertainty 1 0.4909700 Active_QBC 2 0.5273183 Active_random 2 0.5459820 LabelPropagation_rbf 4 0.2064104 LabelSpreading_rbf 5 0.2164875 LabelSpreading_knn 6 0.0424174 LabelPropagation_knn 7 0.0328769 saving the table to latex rank_df_table %&gt;% kable( &quot;latex&quot;, table.envir = &#39;table&#39;, caption=&#39;Ranking of the algorithms with 50% of available labels&#39;, booktabs=T, label=&#39;ranking10&#39;, format.args = list(scientific = FALSE), digits = 3, linesep = &quot;&quot;) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;), full_width = F) %&gt;% readr::write_lines(&#39;./paper/ranking50.tex&#39;) "]]
